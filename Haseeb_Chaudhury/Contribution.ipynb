{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b380d9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\chaud\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import rcParams\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import string\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09012fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Flag</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target          ID                          Date      Flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              User                                               Text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Read the CSV file, providing the correct file path\n",
    "file_path = \"training.1600000.processed.noemoticon.csv\"\n",
    "df = pd.read_csv(file_path, encoding=\"ISO-8859-15\", names=['Target', 'ID', 'Date', 'Flag', 'User', 'Text'])\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b654eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[[\"Text\",\"Target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1491870",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Target\"][data[\"Target\"]==4]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21960c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data[data[\"Target\"]==1].iloc[:int(20000)]\n",
    "data_neg = data[data[\"Target\"]==0].iloc[:int(20000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a822b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_pos, data_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b1c77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Text\"]=data[\"Text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaa27640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chaud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d59a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6c2fdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                love @health4uandpets u guys r best!!\n",
       "800001    im meeting one besties tonight! cant wait!! - ...\n",
       "800002    @darealsunisakim thanks twitter add, sunisa! g...\n",
       "800003    sick really cheap hurts much eat real food plu...\n",
       "800004                      @lovesbrooklyn2 effect everyone\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stop)\n",
    "\n",
    "def clean_words(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda text: clean_words(text))\n",
    "data[\"Text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58cf301e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                   love health4uandpets u guys r best\n",
       "800001    im meeting one besties tonight cant wait  girl...\n",
       "800002    darealsunisakim thanks twitter add sunisa got ...\n",
       "800003    sick really cheap hurts much eat real food plu...\n",
       "800004                       lovesbrooklyn2 effect everyone\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuations = string.punctuation\n",
    "\n",
    "def clean_punc(text):\n",
    "    return text.translate(str.maketrans('','', punctuations))\n",
    "\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda text: clean_punc(text))\n",
    "data[\"Text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c81fda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                   love health4uandpets u guys r best\n",
       "800001    im meting one besties tonight cant wait girl talk\n",
       "800002    darealsunisakim thanks twiter ad sunisa got me...\n",
       "800003    sick realy cheap hurts much eat real fod plus ...\n",
       "800004                         lovesbroklyn2 efect everyone\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_repeats(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda text: clean_repeats(text))\n",
    "data[\"Text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3abb870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                   love health4uandpets u guys r best\n",
       "800001    im meting one besties tonight cant wait girl talk\n",
       "800002    darealsunisakim thanks twiter ad sunisa got me...\n",
       "800003    sick realy cheap hurts much eat real fod plus ...\n",
       "800004                         lovesbroklyn2 efect everyone\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_email(text):\n",
    "    return re.sub('@[^\\s]+',' ', text)\n",
    "\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda text: clean_email(text))\n",
    "data[\"Text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f50a45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                   love health4uandpets u guys r best\n",
       "800001    im meting one besties tonight cant wait girl talk\n",
       "800002    darealsunisakim thanks twiter ad sunisa got me...\n",
       "800003    sick realy cheap hurts much eat real fod plus ...\n",
       "800004                         lovesbroklyn2 efect everyone\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_URLs(text):\n",
    "    return re.sub('((www\\.[^\\s]+) | (https?://[^\\s]+))','',text)\n",
    "\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda text: clean_URLs(text))\n",
    "data[\"Text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d992f750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                    love healthuandpets u guys r best\n",
       "800001    im meting one besties tonight cant wait girl talk\n",
       "800002    darealsunisakim thanks twiter ad sunisa got me...\n",
       "800003    sick realy cheap hurts much eat real fod plus ...\n",
       "800004                          lovesbroklyn efect everyone\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_nums(text):\n",
    "    return re.sub('[0-9]+','',text)\n",
    "\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda text: clean_nums(text))\n",
    "data[\"Text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5174b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000             [love, healthuandpets, u, guys, r, best]\n",
       "800001    [im, meting, one, besties, tonight, cant, wait...\n",
       "800002    [darealsunisakim, thanks, twiter, ad, sunisa, ...\n",
       "800003    [sick, realy, cheap, hurts, much, eat, real, f...\n",
       "800004                      [lovesbroklyn, efect, everyone]\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = RegexpTokenizer(r'\\w+')\n",
    "data[\"Text\"] = data[\"Text\"].apply(token.tokenize)\n",
    "data[\"Text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d3123fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000               [love, healthuandpet, u, guy, r, best]\n",
       "800001    [im, mete, one, besti, tonight, cant, wait, gi...\n",
       "800002    [darealsunisakim, thank, twiter, ad, sunisa, g...\n",
       "800003    [sick, reali, cheap, hurt, much, eat, real, fo...\n",
       "800004                       [lovesbroklyn, efect, everyon]\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    return [st.stem(word) for word in text]\n",
    "\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda text: stemming(text))\n",
    "data[\"Text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa8f9c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chaud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "800000               [love, healthuandpet, u, guy, r, best]\n",
       "800001    [im, mete, one, besti, tonight, cant, wait, gi...\n",
       "800002    [darealsunisakim, thank, twiter, ad, sunisa, g...\n",
       "800003    [sick, reali, cheap, hurt, much, eat, real, fo...\n",
       "800004                       [lovesbroklyn, efect, everyon]\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(text):\n",
    "    return [lm.lemmatize(word) for word in text]\n",
    "\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda text: lemmatizing(text))\n",
    "data[\"Text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20b45c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"Text\"]\n",
    "y = data[\"Target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1201a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 500\n",
    "tok = Tokenizer(num_words=2000)\n",
    "tok.fit_on_texts(X)\n",
    "sequences = tok.texts_to_sequences(X)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "def02292",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(sequences_matrix, y, test_size=0.20, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3be029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_based_model(): #Defined tensorflow_based_model function for training tenforflow based model\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(2000,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer) \n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0557b6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\chaud\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = tensorflow_based_model() # here we are calling the function of created model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01d256ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d667da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "WARNING:tensorflow:From C:\\Users\\chaud\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\chaud\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "360/360 [==============================] - 129s 352ms/step - loss: 0.5969 - accuracy: 0.6730 - val_loss: 0.5370 - val_accuracy: 0.7241\n",
      "Epoch 2/6\n",
      "360/360 [==============================] - 131s 364ms/step - loss: 0.5084 - accuracy: 0.7536 - val_loss: 0.5220 - val_accuracy: 0.7369\n",
      "Epoch 3/6\n",
      "360/360 [==============================] - 128s 356ms/step - loss: 0.4934 - accuracy: 0.7642 - val_loss: 0.5203 - val_accuracy: 0.7419\n",
      "Epoch 4/6\n",
      "360/360 [==============================] - 127s 354ms/step - loss: 0.4818 - accuracy: 0.7689 - val_loss: 0.5245 - val_accuracy: 0.7372\n",
      "Epoch 5/6\n",
      "360/360 [==============================] - 129s 358ms/step - loss: 0.5221 - accuracy: 0.7441 - val_loss: 0.5307 - val_accuracy: 0.7391\n",
      "Epoch 6/6\n",
      "360/360 [==============================] - 131s 364ms/step - loss: 0.4644 - accuracy: 0.7801 - val_loss: 0.5445 - val_accuracy: 0.7344\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,Y_train,batch_size=80,epochs=6, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0266452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 14s 52ms/step - loss: 0.5275 - accuracy: 0.7449\n"
     ]
    }
   ],
   "source": [
    "accr1 = model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6310a02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7448750138282776\n"
     ]
    }
   ],
   "source": [
    "print(accr1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aeaa75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
